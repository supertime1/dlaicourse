{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tensorflow Serving.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/dlaicourse/blob/master/Tensorflow_Serving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XXdEnmVvXdUa"
      },
      "source": [
        "# Train Your Own Model and Serve It With TensorFlow Serving\n",
        "\n",
        "In this notebook, you will train a neural network to classify images of handwritten digits from the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. You will then save the trained model, and serve it using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cSfb9Qd5XdFL"
      },
      "source": [
        "**Warning: This notebook is designed to be run in a Google Colab only**.  It installs packages on the system and requires root access. If you want to run it in a local Jupyter notebook, please proceed with caution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jcLTAmF5Xs2T"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20Deployment/Course%204%20-%20TensorFlow%20Serving/Week%201/Exercises/TFServing_Week1_Exercise.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20Deployment/Course%204%20-%20TensorFlow%20Serving/Week%201/Exercises/TFServing_Week1_Exercise.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2Q8bkjeYTl-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g8r89tTPI-Kb",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XGFJmWjrKttn",
        "outputId": "5a9563d9-7623-401c-e6ea-624cfa5b3e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "â€¢ Using TensorFlow Version: 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pq-214o8SNt0"
      },
      "source": [
        "## Import the MNIST Dataset\n",
        "\n",
        "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains 70,000 grayscale images of the digits 0 through 9. The images show individual digits at a low resolution (28 by 28 pixels). \n",
        "\n",
        "Even though these are really images, we will load them as NumPy arrays and not as binary image objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MqDQO0KCaWS",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AIT-qX0QzLo-",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Scale the values of the arrays below to be between 0.0 and 1.0.\n",
        "train_images = train_images/255.0 # YOUR CODE HERE\n",
        "test_images = test_images/255.0 # YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mIDGu-EEzdKb"
      },
      "source": [
        "In the cell below use the `.reshape` method to resize the arrays to the following sizes:\n",
        "\n",
        "```python\n",
        "train_images.shape: (60000, 28, 28, 1)\n",
        "test_images.shape: (10000, 28, 28, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XsIxeG6BzN4t",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Reshape the arrays below.\n",
        "train_images = np.expand_dims(np.array(train_images), axis=3)# YOUR CODE HERE\n",
        "test_images = np.expand_dims(np.array(test_images), axis=3)# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aUw8ZxigB1Nx",
        "outputId": "5f7b88a5-d604-4f4a-938b-d7fdaab83e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
        "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_images.shape: (60000, 28, 28, 1), of float64\n",
            "test_images.shape: (10000, 28, 28, 1), of float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DcR0OKbOSj0c"
      },
      "source": [
        "## Look at a Sample Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQMs4v_oSo9v",
        "outputId": "25786110-8770-47a5-9203-f36e08db580e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "idx = 42\n",
        "\n",
        "plt.imshow(test_images[idx].reshape(28,28), cmap=plt.cm.binary)\n",
        "plt.title('True Label: {}'.format(test_labels[idx]), fontdict={'size': 16})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEKCAYAAADUyyOuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/4xPFEosK9qBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rn_-9OsPYnDp"
      },
      "source": [
        "## Build a Model\n",
        "\n",
        "In the cell below build a `tf.keras.Sequential` model that can be used to classify the images of the MNIST dataset. Feel free to use the simplest possible CNN. Make sure your model has the correct `input_shape` and the correct number of output units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgMgJJynMbVY",
        "outputId": "bb96e48e-5771-4370-b1cc-83b39737eaa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "# EXERCISE: Create a model.\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Conv2D(8,(1,1),strides=(1,1),activation='relu',input_shape=(28,28,1)),\n",
        "                             tf.keras.layers.BatchNormalization(),\n",
        "                             tf.keras.layers.MaxPooling2D(pool_size=(1,2), strides=(1,2)),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             \n",
        "                             tf.keras.layers.Conv2D(16,(1,3),strides=(1,1),activation='relu'),\n",
        "                             tf.keras.layers.BatchNormalization(),\n",
        "                             tf.keras.layers.MaxPooling2D(pool_size=(1,2),strides=(1,2)),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "                             tf.keras.layers.Conv2D(32, (1, 3), strides=(1, 1),activation='relu'),\n",
        "                             tf.keras.layers.BatchNormalization(),\n",
        "                             tf.keras.layers.MaxPooling2D(pool_size=(1, 2),strides=(1, 2)),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "                             tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.Dense(64, activation='relu'),\n",
        "                             tf.keras.layers.BatchNormalization(),\n",
        "                             tf.keras.layers.Dropout(0.2),\n",
        "                             tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "                             # YOUR CODE HERE\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 8)         16        \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 8)         32        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 28, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 28, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 12, 16)        400       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 28, 12, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 28, 6, 16)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 28, 6, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 4, 32)         1568      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 28, 4, 32)         128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 28, 2, 32)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 28, 2, 32)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1792)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                114752    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 117,866\n",
            "Trainable params: 117,626\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLzXnZT1YvS6"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "In the cell below configure your model for training using the `adam` optimizer, `sparse_categorical_crossentropy` as the loss, and `accuracy` for your metrics. Then train the model for the given number of epochs, using the `train_images` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LTNN0ANGgA36",
        "outputId": "b43d8c7d-d998-4680-fe8b-fd7131d31295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "# EXERCISE: Configure the model for training.\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "# YOUR CODE HERE)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#split training to train and validation\n",
        "train_images, validation_images, train_labels, validation_labels = train_test_split(train_images,train_labels, \n",
        "                                                                                    test_size=0.2, \n",
        "                                                                                    random_state=42)\n",
        "# EXERCISE: Train the model.\n",
        "history = model.fit(ImageDataGenerator().flow(train_images,train_labels,batch_size=32),\n",
        "                    steps_per_epoch=len(train_images)/32,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=ImageDataGenerator().flow(validation_images,validation_labels,batch_size=32),\n",
        "                    validation_steps=len(validation_images)/32)\n",
        "    # YOUR CODE HERE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 1500.0 steps, validate for 375.0 steps\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3445 - accuracy: 0.8938 - val_loss: 0.1566 - val_accuracy: 0.9517\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1929 - accuracy: 0.9401 - val_loss: 0.1298 - val_accuracy: 0.9578\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1621 - accuracy: 0.9501 - val_loss: 0.1268 - val_accuracy: 0.9600\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1496 - accuracy: 0.9534 - val_loss: 0.0909 - val_accuracy: 0.9726\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.1382 - accuracy: 0.9563 - val_loss: 0.0970 - val_accuracy: 0.9712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Er_vrDhf4qu5"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMD387B93f2g",
        "outputId": "3c4f7a43-9768-4baa-9d77-96939551a60a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# EXERCISE: Evaluate the model on the test images.\n",
        "results_eval = model.evaluate(test_images, test_labels, batch_size = 128)# YOUR CODE HERE\n",
        "\n",
        "for metric, value in zip(model.metrics_names, results_eval):\n",
        "    print(metric + ': {:.3}'.format(value))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0856 - accuracy: 0.9737\n",
            "loss: 0.0856\n",
            "accuracy: 0.974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGfmT8M1Yx5y"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9uFDoDW_7HX6",
        "outputId": "95e9310b-9f63-41be-ae7d-dc66b0bb975c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "MODEL_DIR = tempfile.gettempdir()\n",
        "\n",
        "version = 1\n",
        "\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "\n",
        "if os.path.isdir(export_path):\n",
        "    print('\\nAlready saved a model, cleaning up\\n')\n",
        "    !rm -r {export_path}\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "print('\\nexport_path = {}'.format(export_path))\n",
        "!ls -l {export_path}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Already saved a model, cleaning up\n",
            "\n",
            "INFO:tensorflow:Assets written to: /tmp/1/assets\n",
            "\n",
            "export_path = /tmp/1\n",
            "total 480\n",
            "drwxr-xr-x 2 root root   4096 Feb 24 02:37 assets\n",
            "-rw-r--r-- 1 root root 480725 Feb 24 02:37 saved_model.pb\n",
            "drwxr-xr-x 2 root root   4096 Feb 24 02:37 variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KziE3e9tY-hH"
      },
      "source": [
        "## Examine Your Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LU4GDF_aYtfQ",
        "outputId": "931c06b6-cd8a-4851-90b3-ccc2ff0486b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['conv2d_3_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_conv2d_3_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_3'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "WARNING:tensorflow:From /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_3_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AsDTdBGHZAzo"
      },
      "source": [
        "## Add TensorFlow Serving Distribution URI as a Package Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EWg9X2QHlbGS",
        "outputId": "14a44719-7950-4d12-cd0d-fd830c45625c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2943  100  2943    0     0   143k      0 --:--:-- --:--:-- --:--:--  143k\n",
            "OK\n",
            "Hit:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease\n",
            "Hit:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:4 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "107 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4l5XkzqNZNBU"
      },
      "source": [
        "## Install TensorFlow Serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ygwa9AgRloYy",
        "outputId": "38a38c0d-82e4-491b-f5d0-c76a909b3c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tensorflow-model-server is already the newest version (2.1.0).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 107 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qd_PobAKZWW8"
      },
      "source": [
        "## Run the TensorFlow Model Server\n",
        "\n",
        "You will now launch the TensorFlow model server with a bash script. In the cell below use the following parameters when running the TensorFlow model server:\n",
        "\n",
        "* `rest_api_port`: Use port `8501` for your requests.\n",
        "\n",
        "\n",
        "* `model_name`: Use `digits_model` as your model name. \n",
        "\n",
        "\n",
        "* `model_base_path`: Use the environment variable `MODEL_DIR` defined below as the base path to the saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aUgp3vUdU5GS",
        "colab": {}
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kJDhHNJVnaLN",
        "outputId": "93321d49-526e-497e-b953-f84292717110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# EXERCISE: Fill in the missing code below.\n",
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=digits_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 8 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IxbeiOCUUs2z",
        "outputId": "86039501-8a31-469a-f141-e7d88822c073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 223] NET_LOG: Couldn't bind to port 8501\n",
            "[evhttp_server.cc : 63] NET_LOG: Server has not been terminated. Force termination now.\n",
            "[evhttp_server.cc : 258] NET_LOG: Server is not running ...\n",
            "2020-02-24 02:41:41.620448: E tensorflow_serving/model_servers/server.cc:380] Failed to start HTTP Server at localhost:8501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYG5GLDICw0G",
        "colab_type": "code",
        "outputId": "53ecf13c-37cd-4d6e-cc70-5383bdff002f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "def show(idx, title):\n",
        "  plt.figure()\n",
        "  plt.imshow(test_images[idx].reshape(28,28))\n",
        "  plt.axis('off')\n",
        "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
        "\n",
        "import random\n",
        "rando = random.randint(0,len(test_images)-1)\n",
        "show(rando, 'An Example Image: {}'.format(test_labels[rando]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAEcCAYAAAA81qNOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkULg/dVMYQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6mUrIWVRZdNu"
      },
      "source": [
        "## Create JSON Object with Test Images\n",
        "\n",
        "In the cell below construct a JSON object and use the first three images of the testing set (`test_images`) as your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2dsD7KQG1m-R",
        "outputId": "b36540fc-9696-458b-f362-b73b891f8dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# EXERCISE: Create JSON Object\n",
        "data = json.dumps({'signature_name': 'serving_default', 'instances': test_images[0:3].tolist()})\n",
        "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data: {\"signature_name\": \"serving_default\", \"instances\": ...  [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRdyPl4CZ5CU"
      },
      "source": [
        "## Make Inference Request\n",
        "\n",
        "In the cell below, send a predict request as a POST to the server's REST endpoint, and pass it your test data. You should ask the server to give you the latest version of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGvFyuIzW6n6",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Fill in the code below\n",
        "!pip install -q requests\n",
        "\n",
        "import requests\n",
        "headers = {'content-type': \"application/json\"}# YOUR CODE HERE\n",
        "json_response = requests.post('http://localhost:8501/v1/models/digits_model:predict', data=data, headers=headers)# YOUR CODE HERE\n",
        "    \n",
        "predictions = json.loads(json_response.text)['predictions']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FtrFMts_ackX"
      },
      "source": [
        "## Plot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BxQzj34aiDz1",
        "outputId": "1c2f8a19-be42-4ffe-80a4-39d4f4f42fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "plt.figure(figsize=(10,15))\n",
        "\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(test_images[i].reshape(28,28), cmap = plt.cm.binary)\n",
        "    plt.axis('off')\n",
        "    color = 'green' if np.argmax(predictions[i]) == test_labels[i] else 'red'\n",
        "    plt.title('Prediction: {}\\nTrue Label: {}'.format(np.argmax(predictions[i]), test_labels[i]), color=color)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAADRCAYAAADISmjvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAA82h4AACAeTQ8AADAvP8HDJjY5YCXPOAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x1080 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
